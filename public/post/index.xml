<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on CVXR</title>
    <link>/post/</link>
    <description>Recent content in Posts on CVXR</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 02 Nov 2017 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Fastest Mixing Markov Chain</title>
      <link>/post/examples/cvxr_fast-mixing-mc/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/examples/cvxr_fast-mixing-mc/</guid>
      <description>Introduction This example is derived from the results in Boyd, Diaconis, and Xiao (2004), section 2. Let \(\mathcal{G} = (\mathcal{V}, \mathcal{E})\) be a connected graph with vertices \(\mathcal{V} = \{1,\ldots,n\}\) and edges \(\mathcal{E} \subseteq \mathcal{V} \times \mathcal{V}\). Assume that \((i,i) \in \mathcal{E}\) for all \(i = 1,\ldots,n\), and \((i,j) \in \mathcal{E}\) implies \((j,i) \in \mathcal{E}\). Under these conditions, a discrete-time Markov chain on \(\mathcal{V}\) will have the uniform distribution as one of its equilibrium distributions.</description>
    </item>
    
    <item>
      <title>Kelly Gambling</title>
      <link>/post/examples/cvxr_kelly-strategy/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/examples/cvxr_kelly-strategy/</guid>
      <description>Introduction In Kelly gambling (Kelly 1956), we are given the opportunity to bet on \(n\) possible outcomes, which yield a random non-negative return of \(r \in {\mathbf R}_+^n\). The return \(r\) takes on exactly \(K\) values \(r_1,\ldots,r_K\) with known probabilities \(\pi_1,\ldots,\pi_K\). This gamble is repeated over \(T\) periods. In a given period \(t\), let \(b_i \geq 0\) denote the fraction of our wealth bet on outcome \(i\). Assuming the \(n\)th outcome is equivalent to not wagering (it returns one with certainty), the fractions must satisfy \(\sum_{i=1}^n b_i = 1\).</description>
    </item>
    
    <item>
      <title>Largest Ball in a Polyhedron in 2D</title>
      <link>/post/examples/cvxr_2d_ball/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/examples/cvxr_2d_ball/</guid>
      <description>Problem The following is a problem from Boyd and Vandenberghe (2004), section 4.3.1.
Find the largest Euclidean ball (i.e.Â its center and radius) that lies in a polyhedron described by affine inequalites:
\[ P = {x : a_i&amp;#39;*x &amp;lt;= b_i, i=1,...,m} \]
where x is in \({\mathbf R}^2\).
We define variables that determine the polyhedron.
a1 &amp;lt;- matrix(c(2,1)) a2 &amp;lt;- matrix(c(2,-1)) a3 &amp;lt;- matrix(c(-1,2)) a4 &amp;lt;- matrix(c(-1,-2)) b &amp;lt;- rep(1,4) Next, we formulate the CVXR problem.</description>
    </item>
    
    <item>
      <title>Log-Concave Distribution Estimation</title>
      <link>/post/examples/cvxr_log-concave/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/examples/cvxr_log-concave/</guid>
      <description>Introduction Let \(n = 1\) and suppose \(x_i\) are i.i.d. samples from a log-concave discrete distribution on \(\{0,\ldots,K\}\) for some \(K \in {\mathbf Z}_+\). Define \(p_k := {\mathbf {Prob}}(X = k)\) to be the probability mass function. One method for estimating \(\{p_0,\ldots,p_K\}\) is to maximize the log-likelihood function subject to a log-concavity constraint , i.e.,
\[\begin{array}{ll} \underset{p}{\mbox{maximize}} &amp;amp; \sum_{k=0}^K M_k\log p_k \\ \mbox{subject to} &amp;amp; p \geq 0, \quad \sum_{k=0}^K p_k = 1, \\ &amp;amp; p_k \geq \sqrt{p_{k-1}p_{k+1}}, \quad k = 1,\ldots,K-1, \end{array}\] where \(p \in {\mathbf R}^{K+1}\) is our variable of interest and \(M_k\) represents the number of observations equal to \(k\), so that \(\sum_{k=0}^K M_k = m\).</description>
    </item>
    
    <item>
      <title>Near Isotonic and Near Convex Regression</title>
      <link>/post/examples/cvxr_near-isotonic-and-near-convex-regression/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/examples/cvxr_near-isotonic-and-near-convex-regression/</guid>
      <description>Given a set of data points \(y \in {\mathbf R}^m\), R. J. Tibshirani, Hoefling, and Tibshirani (2011) fit a nearly-isotonic approximation \(\beta \in {\mathbf R}^m\) by solving
\[\begin{array}{ll} \underset{\beta}{\mbox{minimize}} &amp;amp; \frac{1}{2}\sum_{i=1}^m (y_i - \beta_i)^2 + \lambda \sum_{i=1}^{m-1}(\beta_i - \beta_{i+1})_+, \end{array}\] where \(\lambda \geq 0\) is a penalty parameter and \(x_+ =\max(x,0)\). This can be directly formulated in CVXR. As an example, we use global warming data from the Carbon Dioxide Information Analysis Center (CDIAC).</description>
    </item>
    
    <item>
      <title>Portfolio Optimization</title>
      <link>/post/examples/cvxr_portfolio-optimization/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/examples/cvxr_portfolio-optimization/</guid>
      <description>Introduction In this example, we solve the Markowitz portfolio problem under various constraints (Markowitz 1952; Roy 1952; Lobo, Fazel, and Boyd 2007).
We have \(n\) assets or stocks in our portfolio and must determine the amount of money to invest in each. Let \(w_i\) denote the fraction of our budget invested in asset \(i = 1,\ldots,m\), and let \(r_i\) be the returns (, fractional change in price) over the period of interest.</description>
    </item>
    
    <item>
      <title>Saturating Hinges Fit</title>
      <link>/post/examples/cvxr_saturating_hinges/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/examples/cvxr_saturating_hinges/</guid>
      <description>Introduction The following example comes from work on saturating splines in N. Boyd et al. (2016). Adaptive regression splines are commonly used in statistical modeling, but the instability they exhibit beyond their boundary knots makes extrapolation dangerous. One way to correct this issue for linear splines is to require they saturate: remain constant outside their boundary. This problem can be solved using a heuristic that is an extension of lasso regression, producing a weighted sum of hinge functions, which we call a saturating hinge.</description>
    </item>
    
    <item>
      <title>Sparse Inverse Covariance Estimation</title>
      <link>/post/examples/cvxr_sparse_inverse_covariance_estimation/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/examples/cvxr_sparse_inverse_covariance_estimation/</guid>
      <description>Introduction Assume we are given i.i.d. observations \(x_i \sim N(0,\Sigma)\) for \(i = 1,\ldots,m\), and the covariance matrix \(\Sigma \in {\mathbf S}_+^n\), the set of symmetric positive semidefinite matrices, has a sparse inverse \(S = \Sigma^{-1}\). Let \(Q = \frac{1}{m-1}\sum_{i=1}^m (x_i - \bar x)(x_i - \bar x)^T\) be our sample covariance. One way to estimate \(\Sigma\) is to maximize the log-likelihood with the prior knowledge that \(S\) is sparse (Friedman, Hastie, and Tibshirani 2008), which amounts to the optimization problem:</description>
    </item>
    
    <item>
      <title>The Catenary Problem</title>
      <link>/post/examples/cvxr_catenary/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/examples/cvxr_catenary/</guid>
      <description>Introduction A chain with uniformly distributed mass hangs from the endpoints \((0,1)\) and \((1,1)\) on a 2-D plane. Gravitational force acts in the negative \(y\) direction. Our goal is to find the shape of the chain in equilibrium, which is equivalent to determining the \((x,y)\) coordinates of every point along its curve when its potential energy is minimized.
This is the famous catenary problem.
 A Discrete Version To formulate as an optimization problem, we parameterize the chain by its arc length and divide it into \(m\) discrete links.</description>
    </item>
    
    <item>
      <title>Tutorial Examples</title>
      <link>/post/cvxr_examples/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/cvxr_examples/</guid>
      <description>Many of the examples here were ported from the CVXPY site, although we have added some new ones as well.
 Largest Euclidean ball in a 2D polyhedron Catenary Problem Isotonic Regression Near Isotonic and Near Convex Regression Direct Standardization Log-Concave Density Estimation Sparse Inverse Covariance Estimation Saturating Hinges Kelly Gambling Fastest Mixing Markov Chain Portfolio Optimization  References  </description>
    </item>
    
    <item>
      <title>Direct Standardization</title>
      <link>/post/examples/cvxr_direct-standardization/</link>
      <pubDate>Wed, 01 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/examples/cvxr_direct-standardization/</guid>
      <description>Introduction Consider a set of observations \((x_i,y_i)\) drawn non-uniformly from an unknown distribution. We know the expected value of the columns of \(X\), denoted by \(b \in {\mathbf R}^n\), and want to estimate the true distribution of \(y\). This situation may arise, for instance, if we wish to analyze the health of a population based on a sample skewed toward young males, knowing the average population-level sex, age, etc. The empirical distribution that places equal probability \(1/m\) on each \(y_i\) is not a good estimate.</description>
    </item>
    
    <item>
      <title>Isotonic Regression</title>
      <link>/post/examples/cvxr_isotonic-regression/</link>
      <pubDate>Wed, 01 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/examples/cvxr_isotonic-regression/</guid>
      <description>Introduction Isotonic regression is regression with monotonic constraints. There are several packages in R to fit isotonic regression models. In this example, we consider isotone which uses a pooled-adjacent-violators algorithm (PAVA) and active set methods to perform the fit.
 Pituitary Data Example We will use data from the isotone package (see de Leeuw, Hornik, and Mair (2009)) on the size of pituitary fissures for 11 subjects between 8 and 14 years of age.</description>
    </item>
    
    <item>
      <title>CVXR Functions</title>
      <link>/post/cvxr_functions/</link>
      <pubDate>Mon, 30 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/cvxr_functions/</guid>
      <description>Functions Here we describe the functions that can be applied to CVXR expressions. CVXR uses the function information in this section and the Disciplined Convex Programming tools to mark expressions with a sign and curvature.
Operators The infix operators +, -, *, %*%, / are treated as functions. + and - are affine functions. * and / are affine in CVXR because expr1*expr2 and expr1 %*% expr2 are allowed only when one of the expressions is constant and expr1/expr2 is allowed only when expr2 is a scalar constant.</description>
    </item>
    
    <item>
      <title>Convex Optimization in R</title>
      <link>/post/cvxr/</link>
      <pubDate>Mon, 30 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/cvxr/</guid>
      <description>What is CVXR? CVXR is an R package that provides an object-oriented modeling language for convex optimization, similar to CVX, CVXPY, YALMIP, and Convex.jl. It allows the user to formulate convex optimization problems in a natural mathematical syntax rather than the restrictive standard form required by most solvers. The user specifies an objective and set of constraints by combining constants, variables, and parameters using a library of functions with known mathematical properties.</description>
    </item>
    
    <item>
      <title>A Gentle Introduction to `CVXR`</title>
      <link>/post/examples/cvxr_gentle-intro/</link>
      <pubDate>Sun, 29 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/examples/cvxr_gentle-intro/</guid>
      <description>Introduction Welcome to CVXR: a modeling language for describing and solving convex optimization problems that follows the natural, mathematical notation of convex optimization rather than the requirements of any particular solver. The purpose of this document is both to introduce the reader to CVXR and to generate excitement for its possibilities in the field of statistics.
Convex optimization is a powerful and very general tool. As a practical matter, the set of convex optimization problems includes almost every optimization problem that can be solved exactly and efficiently (i.</description>
    </item>
    
    <item>
      <title>Discplined Convex Programming</title>
      <link>/post/cvxr_dcp/</link>
      <pubDate>Sun, 29 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/cvxr_dcp/</guid>
      <description>Disciplined convex programming (DCP) is a system for constructing mathematical expressions with known curvature from a given library of base functions. CVXR uses DCP to ensure that the specified optimization problems are convex.
This section of the tutorial explains the rules of DCP and how they are applied by CVXR.
Visit dcp.stanford.edu for a more interactive introduction to DCP.
Expressions Expressions in CVXR are formed from variables, numerical constants such as R vectors and matrices, the standard arithmetic operators +, -, *, %*%, /, and a library of functions.</description>
    </item>
    
    <item>
      <title>Introduction</title>
      <link>/post/examples/cvxr_intro/</link>
      <pubDate>Sun, 29 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/examples/cvxr_intro/</guid>
      <description>Consider a simple linear regression problem where it is desired to estimate a set of parameters using a least squares criterion.
We generate some synthetic data where we know the model completely, that is
\[ Y = X\beta + \epsilon \]
where \(Y\) is a \(100\times 1\) vector, \(X\) is a \(100\times 10\) matrix, \(\beta = [-4,\ldots ,-1, 0, 1, \ldots, 5]\) is a \(10\times 1\) vector, and \(\epsilon \sim N(0, 1)\).</description>
    </item>
    
  </channel>
</rss>