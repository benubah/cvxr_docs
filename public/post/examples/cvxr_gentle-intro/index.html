<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
	<head lang="en-us">
	<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />
	<meta name="description" content="Disciplined Convex Programming in R">
	<meta name="generator" content="Hugo 0.30.2" />
	
	<title>A Gentle Introduction to `cvxr` &mdash; CVXR</title>
	
	<link rel="stylesheet" href="../../../css/alabaster.css" type="text/css" />
	<link rel="stylesheet" href="../../../css/pygments.css" type="text/css" />

	

	<link rel="shortcut icon" href="../../../favicon.ico" type="image/x-icon"/>
</head>

	<body role="document">
		<div class="document">
			<div class="documentwrapper">
				<div class="bodywrapper">
					<div class="body" role="main">
						
	<h1>A Gentle Introduction to `cvxr`</h1>
	
	<p>Welcome to <code>cvxr</code>: a modeling language for describing and solving convex optimization problems that follows the natural, mathematical notation of convex optimization rather than the requirements of any particular solver. The purpose of this document is both to introduce the reader to <code>cvxr</code> and to generate excitement its possibilities in the field of statistics.</p>
<p>Convex optimization is a powerful and very general tool. As a practical matter, the set of convex optimization problems includes almost every optimization problem that can be solved exactly and efficiently (i.e. without requiring an exhaustive search). If an optimization problem can be solved, it is probably convex. This family of problems becomes even larger if you include those that can be solved <em>approximately</em> and efficiently. To learn more about the mathematics and application of convex optimization, see <a href="http://stanford.edu/~boyd/cvxbook/">Boyd and Vandenberghe 2009</a>.</p>
<p>Convex optimization systems written in other languages are already widely used in practical applications. These include <a href="http://users.isy.liu.se/johanl/yalmip/pmwiki.php?n=Main.WhatIsYALMIP">YALMIP</a> and <a href="http://cvxr.com/cvx/">CVX</a> (Matlab), <a href="http://www.cvxpy.org/en/latest/">CVXPY</a> (Python), and <a href="http://convexjl.readthedocs.org/en/latest/">Convex.jl</a> (Julia). <code>cvxr</code> Shares a lot of its code base with <a href="http://stanford.edu/class/ee364b/projects/2015projects/reports/miller_quigley_zhu_report.pdf">CVXcannon</a> and CVXPY. As far as we know, this is the first full-featured general convex optimization package for R.</p>
<p>One of the great headaches of conventional numerical optimization is the process of deciding which algorithm to use and how to set its parameters. In convex optimization, the particular algorithm matters much less. So while a user of <code>cvxr</code> is still free to choose from a number of different algorithms and to set algorithm parameters as they please, the vast majority of users will not need to do this. <code>cvxr</code> will just work.</p>
<p>The uses for convex optimization in statistics are many and varied. Many parameter-fitting methods are convex, including least-squares, ridge, lasso, and isotonic regression, as well as many other kinds of problems such as maximum entropy or minimum Kullback-Leibler divergence over a finite set.</p>
<p>All of these examples, at least in their most basic forms, are established enough that they already have well-crafted R packages devoted to them. If you use <code>cvxr</code> to solve these problems, it will work. It will probably be slower than a custom-built algorithm—for example glmnet for fitting lasso or ridge regression models—but it will work. However, this is not the true purpose of <code>cvxr</code>. If you want to build a well-established model, you should use one of the well-established packages for doing so. If you want to build your <em>own</em> model—one that is a refinement of an existing method, or perhaps even something that has never been tried before—then <code>cvxr</code> is the place to do it. The advantage of <code>cvxr</code> over glmnet and the like comes from its flexibility: A few lines of code can transform a problem from commonplace to state-of-the-art, and can often do the work of an entire package in the process. (We present an example in section 3 where this is literally the case.)</p>
<p>This document is meant to be a complete introduction to the <code>cvxr</code> package. It assumes basic knowledge of convex optimization and statistics as well as proficiency with R. A potential user of <code>cvxr</code> should read all of it, but especially sections 2 and 4. Section 5 can be skimmed and then used as a reference when necessary. The sections may be summarized as follows:</p>
<blockquote>
<p>Section 2: Thoroughly describes the most important aspects of <code>cvxr</code> for a new user to get started with a simple example.</p>
</blockquote>
<blockquote>
<p>Section 3: Presents a sequence of more sophisticated examples which are meant to illustrate the ease with which <code>cvxr</code> can handle a diverse set of problems.</p>
</blockquote>
<blockquote>
<p>Section 4: While the family of convex optimization problems is very large, it nonetheless has severe limitations which you must understand in order to be an effective user of <code>cvxr</code>. This section lays out these limitations and how they are handled through a system called Disciplined Convex Programming (DCP).</p>
</blockquote>
<blockquote>
<p>Section 5: Presents the complete set of functions that have been implemented in <code>cvxr</code> so far along with some of their most essential properties.</p>
</blockquote>
<p>Happy optimizing!</p>
<div id="convex-optimization" class="section level2">
<h2>2 Convex Optimization</h2>
<p>A convex optimization problem has the following form:</p>
<p><span class="math display">\[
\begin{array}{ll} \mbox{minimize} &amp; f_0(x)\\
    \mbox{subject to} &amp; f_i(x) \leq 0, \quad i=1,\ldots,m\\
    &amp; g_i(x) = 0, \quad i=1,\ldots,p
    \end{array}
\]</span></p>
<p>where <span class="math inline">\(x\)</span> is the variable, <span class="math inline">\(f_0\)</span> and <span class="math inline">\(f_1,...,f_m\)</span> are convex and <span class="math inline">\(g_1,...,g_p\)</span> are affine. <span class="math inline">\(f_0\)</span> is called the objective function, <span class="math inline">\(f_i \leq 0\)</span> are called the inequality constraints, and <span class="math inline">\(g_i = 0\)</span> are called the equality constraints.</p>
<p>In CVXR, you will specify convex optimization problems in a more convenient format than the one above.</p>
<p>A convex function is one that is upward curving. A concave function is downward curving. An affine function is flat, and is thus both convex and concave.</p>
<p>A convex optimization problem is one that attempts to minimize a convex function (or maximize a concave function) over a convex set of input points.</p>
<p>You can learn much more about convex optimization via <span class="citation">Boyd and Vandenberghe (2004)</span> as well as the <a href="https://lagunita.stanford.edu/courses/Engineering/CVX101/Winter2014/about">CVX101 MOOC</a>.</p>
</div>
<div id="hello-world" class="section level2">
<h2>3 ‘Hello World’</h2>
<p>We begin with one of the simplest possible problems that presents all three of these features:</p>
<p><span class="math display">\[
\begin{array}{ll}
  \mbox{minimize} &amp; x^2 + y^2 \\
    \mbox{subject to} &amp; x \geq 0, \quad 2x + y = 1
\end{array}
\]</span></p>
<p>with scalar variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. This is a convex optimization problem with objective <span class="math inline">\(f_0(x,y) = x^2 + y^2\)</span> and constraint functions <span class="math inline">\(f_1(x,y) = -x\)</span> and <span class="math inline">\(g_1(x,y) = 2x - y - 1\)</span>.</p>
<p>Note that this problem is simple enough to be solved analytically, so we can confirm that <code>cvxr</code> has produced the correct answer. Here’s how we formulate the problem in <code>cvxr</code>.</p>
<pre class="r"><code># Variables minimized over
x &lt;- Variable(1)
y &lt;- Variable(1)

# Problem definition
objective &lt;- Minimize(x^2 + y^2)
constraints &lt;- list(x &gt;= 0, 2*x + y == 1)
prob2.1 &lt;- Problem(objective, constraints)

# Problem solution
solution2.1 &lt;- solve(prob2.1)
solution2.1$status</code></pre>
<pre><code>## [1] &quot;optimal&quot;</code></pre>
<pre class="r"><code>solution2.1$value</code></pre>
<pre><code>## [1] 0.2</code></pre>
<pre class="r"><code>solution2.1$getValue(x)</code></pre>
<pre><code>## [1] 0.3999978</code></pre>
<pre class="r"><code>solution2.1$getValue(y)</code></pre>
<pre><code>## [1] 0.2000044</code></pre>
<pre class="r"><code># The world says &#39;hi&#39; back.</code></pre>
<p>We now turn to a careful explanation of the code. The first lines create two Variable objects, <code>x</code> and <code>y</code>, both of length 1 (i.e. scalar variables).</p>
<pre class="r"><code>x &lt;- Variable(1)
y &lt;- Variable(1)</code></pre>
<p><code>x</code> and <code>y</code> represent what we are allowed to adjust in our problem in order to obtain the optimal solution. They don’t have values yet, and they won’t until after we solve the problem. For now, they are just placeholders.</p>
<p>Next, we define the problem objective.</p>
<pre class="r"><code>objective &lt;- Minimize(x^2 + y^2)</code></pre>
<p>This call to <code>Minimize()</code> does <em>not</em> return the minimum value of the expression <code>x^2 + y^2</code> the way a call to the native R function <code>min()</code> would do (after all, <code>x</code> and <code>y</code> don’t have values yet). Instead, <code>Minimize()</code> creates an Objective object, which defines the goal of the optimization we will perform, namely to find values for <code>x</code> and <code>y</code> which produce the smallest possible value of <code>x^2 + y^2</code>.</p>
<p>The next line defines two constraints—an inequality constraint and an equality constraint, respectively.</p>
<pre class="r"><code>constraints &lt;- list(x &gt;= 0, 2*x + y == 1)</code></pre>
<p>Again, counter to what you might ordinarily expect, the expression <code>x &gt;= 0</code> does not return <code>TRUE</code> or <code>FALSE</code> the way <code>1.3 &gt;= 0</code> would. Instead, the <code>==</code> and <code>&gt;=</code> operators have been overloaded to return Constraint objects which will be used by the solver to enforce the problem’s constraints. (Without them, the solution to our problem would simply be <span class="math inline">\(x = y = 0\)</span>.)</p>
<p>Next, we define our Problem object, which takes our Objective object and our two Constraint objects as inputs.</p>
<pre class="r"><code>prob2.1 &lt;- Problem(objective, constraints)</code></pre>
<p>Problem objects are very flexible in that they can have 0 or more Constraints, and their Objective can be to <code>Minimize()</code> a convex expression (as shown above) <em>or</em> to <code>Maximize()</code> a concave expression. You can even create a Problem object with no Objective, in which case the corresponding problem is called a “feasibility problem,” where the implicit objective is simply to find out whether or not it is possible to simultaneously satisfy all of the given constraints.</p>
<p>The call to <code>Problem()</code> still does not actually <em>solve</em> our optimization problem. That only happens with the call to <code>Solve()</code>.</p>
<pre class="r"><code>solution2.1 &lt;- solve(prob2.1)</code></pre>
<p>Behind the scenes, this call translates the problem into a format that a convex solver can understand, feeds the problem to the solver, and then returns the results to a Solution object. For this problem, the Solution object will contain among other things the optimal value of the objective function <code>x^2 + y^2</code>, values for <code>x</code> and <code>y</code> that achieve that optimal objective value, and some accompanying metadata such as <code>solution2.1$status</code>, which confirms that the solution was indeed <code>&quot;optimal&quot;</code>.</p>
<pre class="r"><code>solution2.1$status</code></pre>
<pre><code>## [1] &quot;optimal&quot;</code></pre>
<pre class="r"><code>solution2.1$value</code></pre>
<pre><code>## [1] 0.2</code></pre>
<pre class="r"><code>solution2.1$getValue(x)</code></pre>
<pre><code>## [1] 0.3999978</code></pre>
<pre class="r"><code>solution2.1$getValue(y)</code></pre>
<pre><code>## [1] 0.2000044</code></pre>
<p>In general when you apply the <code>solve()</code> method to a Problem, several things can happen:</p>
<ol style="list-style-type: decimal">
<li><p><code>solution$status == &quot;optimal&quot;</code>: The problem is solved. Values for the optimization variables are found which satisfy all of the constraints and minimize the objective.</p></li>
<li><p><code>solution$status == &quot;infeasible&quot;</code>: The problem was <em>not</em> solved because no combination of input variables exists that can satisfy all of the constraints. For a trivial example of when this might happen, consider a problem with optimization variable <code>x</code>, and constraints <code>x &gt;= 1</code> and <code>x &lt;= 0</code>. Obviously, no value of <code>x</code> exists that can satisfy both constraints. In this case, <code>solution$opt.val</code> is <code>+Inf</code> for a minimization problem and <code>-Inf</code> for a maximization problem, indicating infinite dissatisfaction with the result. No values are returned for the input variables.</p></li>
<li><p><code>solution$status == &quot;unbounded&quot;</code>: The problem was <em>not</em> solved because the Objective can be made arbitrarily small for a minimization problem or arbitrarily large for a maximization problem. Hence there is no optimal solution because for any given solution it is always possible to find something even more optimal. In this case, <code>solution$opt.val</code> is <code>-Inf</code> for a minimization problem and <code>+Inf</code> for a maximization problem, indicating infinite satisfaction with the result. Again, no values are returned for the input variables.</p></li>
</ol>
<div id="modifying-a-cvxr-problem" class="section level3">
<h3>2.2 Modifying a CVXR Problem</h3>
<p>Like any normal R object, the <code>Problem</code>, <code>Objective</code>, <code>Constraint</code>, and <code>Solution</code> objects can all be modified and computed upon after creation. Here is an example where we modify the problem we created above by changing its objective and adding a constraint, print the modified problem, check whether it is still convex, and then solve the modified problem:</p>
<pre class="r"><code># Modify the problem from example 1
prob2.2 &lt;- prob2.1
prob2.2@objective &lt;- Minimize(x^2 + y^2 + abs(x-y))
prob2.2@constraints &lt;- c(prob2.2@constraints, y &lt;= 1)

# Solve the modified problem
solution2.2 &lt;- solve(prob2.2)

# Examine the solution
solution2.2$status</code></pre>
<pre><code>## [1] &quot;optimal&quot;</code></pre>
<pre class="r"><code>solution2.2$value</code></pre>
<pre><code>## [1] 0.2222222</code></pre>
<pre class="r"><code>solution2.2$getValue(x)</code></pre>
<pre><code>## [1] 0.3333333</code></pre>
<pre class="r"><code>solution2.2$getValue(y)</code></pre>
<pre><code>## [1] 0.3333333</code></pre>
</div>
<div id="an-invalid-problem" class="section level3">
<h3>2.3 An Invalid Problem</h3>
<p>Unfortunately, you can’t just type any arbitrary problem you like into <code>cvxr</code>. There are severe restrictions on what kinds of problems can be handled. For example, if we tried to `Maximize()’ the objective from example 2.1, we get an error:</p>
<pre class="r"><code>prob2.3 &lt;- prob2.1
prob2.3@objective &lt;- Maximize(x^2 + y^2)
solve(prob2.3)</code></pre>
<pre><code>## Error in cvxr::psolve(a, b, ...): Problem does not follow DCP rules.</code></pre>
<p>We would get a similar error if we tried to add the constraint <code>norm2(x) == 1</code>. This is because <code>cvxr</code> uses a strict set of rules called Disciplined Convex Programming (DCP) to evaluate the convexity of any given problem. If you follow these rules, you are guaranteed that your problem is convex. If you don’t follow these rules, <code>cvxr</code> will throw an exception. See section 5 for a complete description of DCP.</p>
</div>
</div>
<div id="introductory-examples" class="section level2">
<h2>3 Introductory Examples</h2>
<p>We begin by showing what a standard linear regression problem looks like in <code>cvxr</code>:</p>
<div id="ordinary-least-squares" class="section level3">
<h3>3.1 Ordinary Least Squares</h3>
<p>For illustration, we generate some synthetic data for use in this example.</p>
<pre><code>## 
## Attaching package: &#39;MASS&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:cvxr&#39;:
## 
##     huber</code></pre>
<pre class="r"><code>beta &lt;- Variable(m)
objective &lt;- Minimize(sum_squares(y - X %*% beta))
prob3.1 &lt;- Problem(objective)</code></pre>
<p>Here, <code>y</code> is the response, <code>X</code> is the matrix of predictors, <code>n</code> is the number of observations, and <code>beta</code> is a vector of coefficients on the predictors. The Ordinary Least-Squares (OLS) solution for <code>beta</code> minimizes <code>1/n</code> times the the <span class="math inline">\(l_2\)</span>-norm of the residuals (i.e. the root-mean-squared error). As we can see below, <code>cvxr</code>’s solution matches the solution obtained by using <code>lm</code>.</p>
<pre class="r"><code>CVXR_solution3.1 &lt;- solve(prob3.1)
lm_solution3.1 &lt;- lm(y ~ 0 + X)</code></pre>
<p><img src="../../../post/examples/cvxr_gentle_intro_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>Obviously, if all you want to do is least-squares linear regression, you should simply use <code>lm</code>. The chief advantage of <code>cvxr</code> is its flexibility, as we will demonstrate with the rest of section 3.</p>
</div>
<div id="non-negative-least-squares" class="section level3">
<h3>3.2 Non-Negative Least Squares</h3>
<p>Looking at Example 3.1, you may notice that the OLS regression problem has an objective, but no constraints. In many contexts, we can greatly improve our model by constraining the solution to reflect our prior knowledge. For example, we may know that the coefficients <code>beta</code> must be non-negative.</p>
<pre class="r"><code>prob3.2 &lt;- prob3.1
prob3.2@constraints &lt;- list(beta &gt;= 0)
solution3.2 &lt;- solve(prob3.2)</code></pre>
<p><img src="../../../post/examples/cvxr_gentle_intro_files/figure-html/unnamed-chunk-16-1.png" width="700" /></p>
<p>As we can see in the figure above, adding that one constraint produced a massive improvement in the accuracy of the estimates. Not only are the non-negative least-squares estimates much closer to the true signal than the OLS estimates, they have even managed to recover the correct sparsity structure in this case.</p>
<p>Like with OLS, there are already R packages available which implement non-negative least squares, such as the package <code>nnls</code>. But that is actually an excellent demonstration of the power of <code>cvxr</code>: A single line of code here, namely <code>prob3.2$constraints &lt;- list(beta &gt;= 0)</code>, is doing the work of an entire package.</p>
</div>
<div id="support-vector-classifiers" class="section level3">
<h3>3.3 Support Vector Classifiers</h3>
<p>Another common statistical tool is the support vector classifier (SVC). The SVC is an affine function (hyperplane) that separates two sets of points by the widest margin. When the sets are not linearly separable, the SVC is determined by a trade-off between the width of the margin and the number of points that are misclassified.</p>
<p>For the binary case, where the response <span class="math inline">\(y_i \in \{-1,1\}\)</span>, the SVC is obtained by solving</p>
<p><span class="math display">\[
\begin{array}{ll}
  \mbox{minimize} &amp; \frac{1}{2}\Vert\beta\Vert^2 + C\sum_{i=1}^m \xi_i  \\
    \mbox{subject to} &amp; \xi_i \geq 0, \quad y_i(x_i^T\beta + \beta_0) \geq 1 - \xi_i, \quad i = 1,\ldots,m
\end{array}
\]</span></p>
<p>with variables <span class="math inline">\((\beta,\xi)\)</span>. Here, <span class="math inline">\(\xi\)</span> is the amount by which a point can violate the separating hyperplane, and <span class="math inline">\(C &gt; 0\)</span> is a user-chosen penalty on the total violation. As <span class="math inline">\(C\)</span> increases, fewer misclassifications will be allowed.</p>
<p>Below, we fit a SVC in <code>cvxr</code> with <span class="math inline">\(C = 10\)</span>.</p>
<pre class="r"><code>## Generate data
set.seed(10)
n &lt;- 2
m &lt;- 50

X &lt;- matrix(rnorm(m*n), nrow = m, ncol = n)
y &lt;- c(rep(-1, m/2), rep(1, m/2))
X[y == 1,] = X[y == 1,] + 1</code></pre>
<pre class="r"><code>## Define variables
cost &lt;- 10
beta0 &lt;- Variable()
beta &lt;- Variable(n)
slack &lt;- Variable(m)

# Form problem
objective &lt;- (1/2) * sum_squares(vstack(beta, beta0)) + cost * sum(slack)
constraints &lt;- list(y * (X %*% beta + beta0) &gt;= 1 - slack, slack &gt;= 0)
prob3.3 &lt;- Problem(Minimize(objective), constraints)
solution3.3 &lt;- solve(prob3.3)</code></pre>
<p><img src="../../../post/examples/cvxr_gentle_intro_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
</div>
</div>
<div id="disciplined-convex-programming-dcp" class="section level2">
<h2>4 Disciplined Convex Programming (DCP)</h2>
<p>Disciplined convex programming (DCP) is a system for constructing mathematical expressions with known curvature from a given library of base functions. <code>cvxr</code> uses DCP to ensure that the specified optimization problems are convex.</p>
<p>You may also find it helpful to read about the DCP rules are applied in other languages such as <a href="http://www.cvxpy.org/en/latest/tutorial/dcp/index.html">Python</a>, <a href="http://cvxr.com/cvx/doc/dcp.html#">Matlab</a>, and <a href="http://convexjl.readthedocs.io/en/latest/types.html">Julia</a>.</p>
<p><code>cvxr</code> implements the same rules.</p>
</div>
<div id="dcp-functions" class="section level2">
<h2>5. DCP functions</h2>
<p>The set of DCP functions are described in <a href="../../../post/cvxr_functions/">Function Reference</a>.</p>
</div>
<div id="references" class="section level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references">
<div id="ref-BoydVandenberghe:2004">
<p>Boyd, S., and L. Vandenberghe. 2004. <em>Convex Optimization</em>. Cambridge University Press.</p>
</div>
</div>
</div>



						<div id="disqus_thread"></div>
<script>
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "cvxr" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
					</div>
				</div>
			</div>
			
			<div class="sphinxsidebar" role="navigation" aria-label="main navigation">
	<div class="sphinxsidebarwrapper">
		<p class="logo">
			<a href="../../../">
				<img class="logo" src="../../../favicon.ico" alt="Logo"/>
				<h1 class="logo logo-name"></h1>
			</a>
		</p>
		
		<p class="blurb">Disciplined Convex Programming in R</p>

		

	<p>
		<iframe src="https://ghbtns.com/github-btn.html?user=bnaras&repo=cvxr_docs&type=watch&count=true&size=large"
		allowtransparency="true" frameborder="0" scrolling="0" width="200px" height="35px"></iframe>
	</p>

	

	
		

		

<h3>Navigation</h3>
<ul>
	
	<li class="toctree-l1">
		<a class="reference internal" href="https://github.com/anqif/cvxr">Github</a>
	</li>
	
	<li class="toctree-l1">
		<a class="reference internal" href="../../../post/examples/cvxr_intro/">A Quick Intro</a>
	</li>
	
	<li class="toctree-l1">
		<a class="reference internal" href="../../../post/examples/cvxr_gentle-intro/">A Longer Intro</a>
	</li>
	
	<li class="toctree-l1">
		<a class="reference internal" href="../../../post/cvxr_examples/">Tutorial Examples</a>
	</li>
	
	<li class="toctree-l1">
		<a class="reference internal" href="../../../post/cvxr_dcp/">DCP</a>
	</li>
	
	<li class="toctree-l1">
		<a class="reference internal" href="../../../post/cvxr_functions/">Function Reference</a>
	</li>
	
</ul>


		<h3>Related Topics</h3>
<ul>
  <li><a href="../../../">Documentation overview</a><ul>
  <li>Previous: <a href="../../../post/cvxr_dcp/" title="Discplined Convex Programming">Discplined Convex Programming</a></li>
  <li>Next: <a href="../../../post/cvxr/" title="Convex Optimization in R">Convex Optimization in R</a></li>
</ul>

	</div>
</div>
<div class="clearer"></div>
</div>
			<script type="text/javascript" src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


			

			

			
		</div>
	</body>
</html>