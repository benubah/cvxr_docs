---
title: Introduction
author: Anqi Fu and Balasubramanian Narasimhan
date: '2017-10-29'
categories:
  - Simple Least Squares
slug: cvxr_intro
---

Consider a simple linear regression problem where it is desired to
estimate a set of parameters using a least squares criterion. 

We generate some synthetic data where we know the model completely,
that is

$$ Y = X\beta + \epsilon $$ 

where $Y$ is a $100\times 1$ vector, $X$ is a $100\times 10$ matrix,
$\beta = -4,\ldots ,-1, 0, 1, \ldots 5$ and $\epsilon \sim  N(0, 1).$


```{r}
set.seed(123)

n <- 100
p <- 10
beta <- -4:5 # beta is just -4 through 5.

X <- matrix(rnorm(n * p), nrow=n)
colnames(X) <- paste0("beta_", beta)
Y <- X %*% beta + rnorm(n)
```

Given the data $X$ and $Y$, we can estimate the $\beta$ using `lm`
function in R that fits a standard regression model.

```{r}
ls.model <- lm(Y ~ 0 + X) ## There is no intercept in our model above
m <- matrix(coef(ls.model), ncol = 1)
rownames(m) <- paste0("$\\beta_{", 1:p, "}$")
library(kableExtra)
knitr::kable(m, format = "html") %>%
    kable_styling("striped") %>%
    column_spec(1, background = "#ececec") %>%
    column_spec(2, background = "#ececec")

```

These are the least-squares estimates and can be seen to be reasonably
close to the original $\beta$ values -4 through 5.

## The `cvxr` formulation

The `cvxr` formulation states the above as an optimization problem:

$$
  \begin{array}{ll}
    \underset{\beta}{\mbox{minimize}} & \|y - X\beta\|_2^2,
  \end{array}
$$
which directly translates into a problem that `cvxr` can solve as shown
in the steps below.

- Step 0. Load the `cvxr` library

```{r}
suppressWarnings(library(cvxr, warn.conflicts=FALSE))
```

- Step 1. Define the variable to be estimated

```{r}
betaHat <- Variable(p)
```

- Step 2. Define the objective to be optimized

```{r}
objective <- Minimize(sum((Y - X %*% betaHat)^2))
```
Notice how the objective is specified using functions such as `sum`,
`*%*` and `^`, that are familiar to R users despite that fact that
`betaHat` is no ordinary R expression but a `cvxr` expression.

- Step 3. Create a problem to solve

```{r}
problem <- Problem(objective)
```

- Step 4. Solve it!

```{r}
result <- solve(problem)
```

- Step 5. Extract solution and objective value

```{r, echo = FALSE}
solution <- result$getValue(betaHat)
cat(sprintf("Objective value: %f\n", result$value))
```

We can indeed satisfy ourselves that the results we get matches that
from `lm`.

```{r}
m <- cbind(result$getValue(betaHat), coef(ls.model))
colnames(m) <- c("cvxr est.", "lm est.")
rownames(m) <- paste0("$\\beta_{", 1:p, "}$")
knitr::kable(m, format = "html") %>%
    kable_styling("striped") %>%
    column_spec(1, background = "#ececec") %>%
    column_spec(2, background = "#ececec") %>%
    column_spec(3, background = "#ececec")
```

## Wait a minute! What have we gained?

On the surface it appears that we have replaced one call to `lm` with
at least five or six lines of new R code. On top of that, the code
actually runs slower and so it is not clear what was really achieved.

So suppose we knew for a fact that the $\beta$s were increasing and we
wish to take this fact into account. This is a special case
of
[isotonic regression](https://en.wikipedia.org/wiki/Isotonic_regression) and
`lm` would no longer do the job. One would have to find a package such
as `isotone` to perform the fit.

The modified problem merely requires the addition of a constraint to the
problem definition; no other modification is necessary. 

```{r}
problem <- Problem(objective, constraints = list(betaHat >= 0))
result <- solve(problem)
m <- matrix(result$getValue(betaHat), ncol = 1)
rownames(m) <- paste0("$\\beta_{", 1:p, "}$")
knitr::kable(m, format = "html") %>%
    kable_styling("striped") %>%
    column_spec(1, background = "#ececec") %>%
    column_spec(2, background = "#ececec")
```

This example demonstrates the chief advantage of `cvxr`:
flexibility. Users can quickly modify and re-solve a problem, making
our package ideal for prototyping new statistical methods. Its syntax
is simple and mathematically intuitive. Furthermore, `cvxr` combines
seamlessly with native R code as well as several popular packages,
allowing it to be incorporated easily into a larger analytical
framework. The user is free to construct statistical estimators that
are solutions to a convex optimization problem where there may not be
a closed form solution or even an implementation. Such solutions can
then be combined with resampling techniques like the bootstrap to estimate
variability. 

## Further Reading

We hope we have vetted your appetite. 

A number of [tutorial examples](/post/cvxr_examples/) are available to
study and mimic. 

## References




